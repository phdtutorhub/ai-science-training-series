I was in group 3 chat room.  There were 4 persons in the room but only Damyn Chipman and I were discussing.  I suggested that LLM is useful for concrete use cases and may not be as useful in abstract and nebulous situations.  We know that LLM is used in driverless cars and unmanned drones.  Although the technology is not perfect yet, the idea basically works.  However if we ask ChatGPT to vote on a presidential candidate, 50% of Americans will say its answer is wrong no matter what the answer is.  So half the people will say that LLM does not work simply because they cannot agree on a "right" answer.  It is not a problem with LLM per se but with the subjective nature of the topic.  LLM can only source information from the data set.  If the data set consists of controversial information, LLM will simply pick a side based on probability and statistics.  LLM can weave through complex data to draw a conclusion but will not be able to transcend human knowledge or to glean value and meaning from it.  It is why LLM will tend to work well in concrete cases where the asnwers are black and white and when value and meaning are not the concerns.  Examples of concrete cases are the laws of physics, mathematics and computer coding.  In terms of evaluating LLM models, the usual criteria such as correctness, efficiency and scalability are good choices.  As per correctness, as I said before, it only makes sense if we can measure it against a set of concrete standards.  Topics such as ethics, politics and religions do not have any universally agreeable standards for correctness.  The answer really depends on whom you ask.  Therefore LLM will have a hard time delivering "correct" answers on these abstract topics.
